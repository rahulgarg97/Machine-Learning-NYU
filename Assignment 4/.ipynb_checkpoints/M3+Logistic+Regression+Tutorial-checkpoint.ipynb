{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Tutorial: UCI Wisconsin Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will provide you some techniques on the following as a supplement to Assignment 3:\n",
    "* Two Ways to Load Data\n",
    "* Dealing with Missing or Unknown Data\n",
    "* Indexing techniques to select desired attributes\n",
    "* Setting up Logistic Model with Sckit-Learn\n",
    "*  Visualing the weight coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-9a4598cf7f62>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-9a4598cf7f62>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    sklearn.datasets.fetch_kddcup99(*, subset=None, data_home=None, shuffle=False, random_state=None, percent10=True, download_if_missing=True, return_X_y=False)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "sklearn.datasets.fetch_kddcup99(*, subset=None, data_home=None, shuffle=False, random_state=None, percent10=True, download_if_missing=True, return_X_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way you can load data is with the .csv file from the UC Irvine Machine Learning Repository website (https://archive.ics.uci.edu/ml/index.php) with the Pandas framework. This is a more generic way of loading data that is not availiable from the Sci-kit learn libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_bc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way you can load data, if it is availiable in the Sci-kit learn library (see what is availiable: https://scikit-learn.org/stable/datasets/index.html), is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_bc = load_breast_cancer()\n",
    "X=data_bc.data\n",
    "y=data_bc.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will proceed with the data loaded from the .csv file availiable on NYUClasses (data_bc.csv). To get an idea of what the data looks like, you may display the first five entries using the head() function from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown or missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may encounter a dataset with either unknown or missing data -- even both! Here are some ways to deal with it. In many other cases, such as the Iris dataset, unknown or missing data may not occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping\n",
    "df = df.drop('Unnamed: 32', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At times, it would not make sense to drop a whole attribute. For more ways to deal with this problem, you may refer to this guide on how to work on datasets with missing data (https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting from a range\n",
    "\n",
    "# Here, we are selecting attribute 2, 3, 4 (radius mean, texture mean, and perimeter mean). \n",
    "# Recall that in Python, indexing starts at 0.\n",
    "\n",
    "x_labels1 = df.columns[2:5]\n",
    "X = np.array(df[x_labels1].values)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting based on desired label names\n",
    "xlbl= ['perimeter_mean', 'area_mean', 'compactness_mean']\n",
    "x_labels1 = df[xlbl]\n",
    "X1=x_labels1.values\n",
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on indexing and selecting data, you may refer to this link (https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals, y = np.unique(df['diagnosis'].values, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "Xs = scale(X)\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression instance\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the data\n",
    "logreg.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_hat = logreg.predict(Xs)\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy=np.mean(y_hat==y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to tell which attributes contribute heavily in the prediction. This is especially true for this dataset because it has about 30 attributes. It may not be efficient to find those by trial and error mix and match methods. Fortunately, there is a way to visualize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a matrix with all the labels\n",
    "x_labels_w = df.columns[2:]\n",
    "Xw = np.array(df[x_labels_w].values)\n",
    "print(Xw)\n",
    "print(\"The matrix dimensions of Xw is \" + str(Xw.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For plotting in the Jupyter Notebook environment as an inline output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, LogisticRegression() is set on penalty as L2 and C=1.\n",
    "# To simulate no regularization, we will select a large C to minimize regularization to later \n",
    "# show the effect of regularization\n",
    "\n",
    "logreg_w=LogisticRegression(C=1e10)\n",
    "logreg_w.fit(Xw,y)\n",
    "W=logreg_w.coef_\n",
    "W=W.flatten()\n",
    "plt.stem(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this stem plot of coefficient weights, we can conclude from visual inspection that there are some labels that will contribute more heavily (large in absolute values) in the prediction than others. To find those labels, we can do the following. As an example, let us find the top three labels that are most heavily weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1=np.argsort(np.abs(W))[-1]\n",
    "idx2=np.argsort(np.abs(W))[-2]\n",
    "idx3=np.argsort(np.abs(W))[-3]\n",
    "\n",
    "heavy=[x_labels_w[idx1], x_labels_w[idx2],x_labels_w[idx3]]\n",
    "heavy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
